# Gu√≠a de Sustentaci√≥n: √âpocas y Entrenamiento de la Red Neuronal

## üìã Estructura Recomendada para la Presentaci√≥n

### 1. INTRODUCCI√ìN AL ENTRENAMIENTO (2-3 minutos)

**Puntos clave a mencionar:**

- **"Implementamos un MLP (Multi-Layer Perceptron) desde cero, sin usar frameworks como TensorFlow o PyTorch"**
- **"El entrenamiento utiliza el algoritmo de descenso de gradiente estoc√°stico (SGD) con procesamiento por lotes (batch processing)"**
- **"La arquitectura implementada es: Input ‚Üí Capa Oculta ‚Üí Capa de Salida"**

**Ejemplo de arquitectura base:**
- Input: 50 caracter√≠sticas
- Hidden: 30 neuronas
- Output: 5 clases
- Funci√≥n de activaci√≥n: Sigmoid (o ReLU para MNIST)
- Learning rate: 0.01
- Batch size: 32

---

### 2. JUSTIFICACI√ìN DEL N√öMERO DE √âPOCAS (3-4 minutos) ‚≠ê **CR√çTICO**

#### 2.1 Configuraci√≥n de √âpocas Utilizada

**Mencionar las diferentes configuraciones seg√∫n el dataset:**

1. **Entrenamiento Base (Datos Sint√©ticos):**
   - **30 √©pocas** - Configuraci√≥n est√°ndar para validaci√≥n r√°pida
   - Justificaci√≥n: Suficiente para convergencia en datos sint√©ticos estructurados
   - Resultado: Precisi√≥n >95%

2. **MNIST (Dataset Real):**
   - **200 √©pocas** - Configuraci√≥n optimizada para dataset complejo
   - Justificaci√≥n: MNIST requiere m√°s iteraciones para aprender patrones complejos
   - Resultado: **91.80% de precisi√≥n** (supera el 85% requerido)

3. **Datos Sint√©ticos Estructurados (2000 muestras):**
   - **200 √©pocas** - Para asegurar convergencia completa
   - Resultado: **97.75% de precisi√≥n**

#### 2.2 An√°lisis de Escalado con √âpocas

**Mencionar el experimento realizado (gr√°fica `ra1_epochs.png`):**

- **Hip√≥tesis te√≥rica:** El tiempo de entrenamiento debe crecer **linealmente** con el n√∫mero de √©pocas
- **Complejidad:** O(E √ó N √ó (n√óh + h√óc)) donde E = √©pocas
- **Resultado experimental:** Confirmado - el tiempo crece proporcionalmente con las √©pocas
- **Rango probado:** 5, 10, 20, 50, 100 √©pocas

**F√≥rmula clave:**
```
Tiempo_total = √âpocas √ó (N/B) √ó Tiempo_por_batch
```

Donde:
- N = n√∫mero de muestras
- B = batch_size
- Tiempo_por_batch = O(B √ó (n√óh + h√óc))

#### 2.3 ¬øPor qu√© 200 √©pocas para MNIST?

**Argumentos t√©cnicos:**

1. **Curva de aprendizaje:**
   - Las primeras 50 √©pocas: Reducci√≥n r√°pida de p√©rdida
   - √âpocas 50-150: Convergencia gradual
   - √âpocas 150-200: Refinamiento fino para alcanzar >90%

2. **Prevenci√≥n de sobreajuste:**
   - Validaci√≥n en cada √©poca permite monitorear generalizaci√≥n
   - Si la precisi√≥n de validaci√≥n deja de mejorar, se podr√≠a detener antes (early stopping no implementado, pero se monitorea)

3. **Balance tiempo/precisi√≥n:**
   - 100 √©pocas: ~88% precisi√≥n
   - 200 √©pocas: **91.80% precisi√≥n** (objetivo cumplido)
   - M√°s √©pocas: Mejora marginal, no justifica el tiempo adicional

---

### 3. PROCESO DE ENTRENAMIENTO DETALLADO (4-5 minutos) ‚≠ê **CR√çTICO**

#### 3.1 Algoritmo de Entrenamiento

**Describir el proceso paso a paso:**

```python
for epoch in range(epochs):  # E iteraciones
    # 1. Mezclar datos (shuffle)
    indices = np.random.permutation(N)
    
    # 2. Procesar por lotes (batches)
    for batch in batches:  # N/B iteraciones
        # Forward pass
        z1, a1, a2 = forward(X_batch)  # O(B √ó (n√óh + h√óc))
        
        # Calcular p√©rdida
        loss = cross_entropy(a2, y_batch)
        
        # Backward pass (backpropagation)
        dW1, db1, dW2, db2 = backward(X_batch, y_batch, z1, a1, a2)
        
        # Actualizar pesos (SGD)
        W1 = W1 - learning_rate √ó dW1
        W2 = W2 - learning_rate √ó dW2
        # ... (similar para sesgos)
    
    # 3. Evaluar en conjunto de validaci√≥n
    val_accuracy = evaluate(X_val, y_val)
```

#### 3.2 Complejidad Temporal del Entrenamiento

**Derivaci√≥n detallada:**

1. **Por batch:**
   - Forward: O(B √ó (n√óh + h√óc))
   - Backward: O(B √ó (n√óh + h√óc))
   - Update: O(n√óh + h√óc)
   - **Total por batch:** O(B √ó (n√óh + h√óc))

2. **Por √©poca:**
   - N√∫mero de batches: ‚åàN/B‚åâ ‚âà N/B
   - **Total por √©poca:** O((N/B) √ó B √ó (n√óh + h√óc)) = **O(N √ó (n√óh + h√óc))**

3. **Entrenamiento completo:**
   - **Total:** O(E √ó N √ó (n√óh + h√óc))
   - Donde E = √©pocas, N = muestras, n = input_size, h = hidden_size, c = output_size

**Ejemplo num√©rico para MNIST:**
- E = 200 √©pocas
- N = 5000 muestras
- n = 784 (28√ó28 p√≠xeles)
- h = 256 neuronas ocultas
- c = 10 clases
- B = 128 batch_size

Complejidad: O(200 √ó 5000 √ó (784√ó256 + 256√ó10)) ‚âà O(200 √ó 5000 √ó 200,704) operaciones

#### 3.3 Complejidad Espacial

**Memoria durante entrenamiento:**

- **Pesos:** O(n√óh + h√óc) = O(784√ó256 + 256√ó10) ‚âà 200,704 par√°metros
- **Activaciones por batch:** O(B √ó (n + h + c)) = O(128 √ó (784 + 256 + 10)) ‚âà 134,400 valores
- **Gradientes:** O(n√óh + h√óc) ‚âà 200,704 valores
- **Total:** O(n√óh + h√óc + B√ó(n+h+c)) ‚âà O(535,808) valores en memoria

**Ventaja del batch processing:**
- Sin batches: O(N √ó (n + h + c)) = O(5000 √ó 1050) ‚âà 5,250,000 valores
- Con batches: O(B √ó (n + h + c)) = O(128 √ó 1050) ‚âà 134,400 valores
- **Reducci√≥n:** ~39x menos memoria

---

### 4. M√âTRICAS Y RESULTADOS (2-3 minutos)

#### 4.1 Resultados de Precisi√≥n

**Presentar los resultados clave:**

| Dataset | √âpocas | Precisi√≥n | Estado |
|---------|--------|-----------|--------|
| MNIST (5000 muestras) | 200 | **91.80%** | ‚úÖ Supera 85% |
| Datos sint√©ticos (2000) | 200 | **97.75%** | ‚úÖ Supera 85% |
| Datos sint√©ticos (1000) | 100 | **100%** | ‚úÖ Supera 85% |

#### 4.2 Evoluci√≥n de la P√©rdida

**Mencionar el comportamiento t√≠pico:**

- **√âpoca 1:** P√©rdida inicial alta (ej: 2.3 para cross-entropy)
- **√âpocas 1-50:** Reducci√≥n r√°pida (ej: 2.3 ‚Üí 0.8)
- **√âpocas 50-150:** Convergencia gradual (ej: 0.8 ‚Üí 0.3)
- **√âpocas 150-200:** Refinamiento (ej: 0.3 ‚Üí 0.25)
- **Reducci√≥n total:** ~90% de la p√©rdida inicial

#### 4.3 Verificaci√≥n de Correctitud

**Mencionar la verificaci√≥n de gradientes:**

- **M√©todo:** Gradient checking (diferencia finita num√©rica)
- **Error m√°ximo:** 4.46e-06 (muy por debajo de tolerancia 1e-5)
- **Conclusi√≥n:** La implementaci√≥n de backpropagation es **matem√°ticamente correcta**
- **Gr√°fica:** `experiments/results/gradient_validation.png`

---

### 5. DECISIONES T√âCNICAS JUSTIFICADAS (2-3 minutos)

#### 5.1 ¬øPor qu√© Batch Size = 32 o 128?

**Argumentos:**

1. **Balance memoria/velocidad:**
   - Batch peque√±o (8-16): M√°s iteraciones, m√°s actualizaciones de pesos, pero m√°s overhead
   - Batch mediano (32-64): Balance √≥ptimo para la mayor√≠a de casos
   - Batch grande (128-256): Menos iteraciones, mejor aprovechamiento de paralelismo, pero requiere m√°s memoria

2. **Para MNIST:**
   - Batch size = 128: Aprovecha mejor el paralelismo, reduce n√∫mero de iteraciones
   - N√∫mero de batches por √©poca: ‚åà5000/128‚åâ = 40 batches
   - Total de actualizaciones: 200 √©pocas √ó 40 = 8,000 actualizaciones

#### 5.2 ¬øPor qu√© Learning Rate = 0.01?

**Justificaci√≥n:**

- **Muy bajo (0.001):** Convergencia muy lenta, requiere m√°s √©pocas
- **√ìptimo (0.01):** Balance entre velocidad de convergencia y estabilidad
- **Muy alto (0.1):** Puede causar oscilaciones o divergencia

**Evidencia:** Con LR=0.01 y 200 √©pocas, alcanzamos 91.80% en MNIST, confirmando que es una buena elecci√≥n.

#### 5.3 ¬øPor qu√© Shuffle en cada √âpoca?

**Raz√≥n:**

- **Evita sesgo:** Sin shuffle, el modelo podr√≠a aprender el orden de los datos
- **Mejora generalizaci√≥n:** Expone el modelo a diferentes combinaciones de datos en cada √©poca
- **Complejidad:** O(N) por √©poca, despreciable comparado con O(N √ó (n√óh + h√óc)) del entrenamiento

---

### 6. AN√ÅLISIS DE COMPLEJIDAD RELACIONADO CON √âPOCAS (2-3 minutos)

#### 6.1 Escalado Lineal con √âpocas

**Teor√≠a:**
- Si duplicamos las √©pocas, el tiempo de entrenamiento se duplica
- F√≥rmula: T(E) = E √ó T_por_√©poca
- **Complejidad:** O(E) - lineal con √©pocas

**Evidencia experimental:**
- Gr√°fica `ra1_epochs.png` muestra crecimiento lineal
- 5 √©pocas: ~X segundos
- 10 √©pocas: ~2X segundos
- 50 √©pocas: ~10X segundos
- 100 √©pocas: ~20X segundos

#### 6.2 Trade-off √âpocas vs Precisi√≥n

**An√°lisis:**

- **Ley de rendimientos decrecientes:**
  - Primeras √©pocas: Gran mejora de precisi√≥n
  - √âpocas intermedias: Mejora moderada
  - √âpocas finales: Mejora marginal

- **Punto √≥ptimo:**
  - Para MNIST: 200 √©pocas alcanza 91.80%
  - 300 √©pocas podr√≠a dar ~92-93%, pero el tiempo adicional no justifica la mejora marginal

#### 6.3 Comparaci√≥n con Baselines

**k-NN (sin entrenamiento):**
- Entrenamiento: O(1) - solo almacena datos
- Predicci√≥n: O(N √ó d) donde N = muestras de entrenamiento
- **Ventaja MLP:** Una vez entrenado, predicci√≥n O(B √ó (n√óh + h√óc)) es mucho m√°s r√°pida

---

### 7. POSIBLES PREGUNTAS DEL DOCENTE Y RESPUESTAS

#### P: "¬øPor qu√© no usaron early stopping?"

**R:** 
- Implementamos monitoreo de validaci√≥n en cada √©poca
- Early stopping es una optimizaci√≥n que podr√≠amos agregar
- Para este proyecto, 200 √©pocas garantizan convergencia completa
- El objetivo era validar la implementaci√≥n, no optimizar tiempo

#### P: "¬øC√≥mo justifican que 200 √©pocas es suficiente?"

**R:**
- Observamos la curva de p√©rdida: despu√©s de 150 √©pocas, la mejora es marginal
- La precisi√≥n de validaci√≥n se estabiliza alrededor de 91-92%
- M√°s √©pocas no mejoran significativamente (ley de rendimientos decrecientes)
- El objetivo del 85% se alcanza consistentemente

#### P: "¬øQu√© pasar√≠a si aumentan las √©pocas a 500?"

**R:**
- **Tiempo:** Se triplicar√≠a aproximadamente (escalado lineal)
- **Precisi√≥n:** Mejora marginal (~1-2% adicional posible)
- **Riesgo:** Posible sobreajuste si no hay regularizaci√≥n
- **Conclusi√≥n:** No es eficiente, 200 √©pocas es el punto √≥ptimo

#### P: "¬øC√≥mo afecta el batch size al n√∫mero de √©pocas necesarias?"

**R:**
- Batch peque√±o: M√°s actualizaciones por √©poca, puede converger en menos √©pocas, pero m√°s lento por √©poca
- Batch grande: Menos actualizaciones por √©poca, puede requerir m√°s √©pocas, pero m√°s r√°pido por √©poca
- **En la pr√°ctica:** Para nuestro caso, batch_size=128 con 200 √©pocas es √≥ptimo

#### P: "¬øCu√°l es la complejidad total del entrenamiento?"

**R:**
- **Temporal:** O(E √ó N √ó (n√óh + h√óc))
  - E = √©pocas (200)
  - N = muestras (5000)
  - n√óh + h√óc = par√°metros (~200,704)
- **Espacial:** O(n√óh + h√óc + B√ó(n+h+c))
  - Pesos + activaciones por batch

---

### 8. DEMOSTRACI√ìN PR√ÅCTICA (Si se solicita)

#### 8.1 Ejecutar Entrenamiento

```bash
# Entrenamiento base (30 √©pocas)
python train_mlp.py

# Prueba de precisi√≥n con MNIST (200 √©pocas)
python test_accuracy.py
```

#### 8.2 Mostrar Resultados

- Mostrar la salida del entrenamiento con las m√©tricas por √©poca
- Mostrar la gr√°fica de tiempo vs √©pocas (`ra1_epochs.png`)
- Mostrar la gr√°fica de validaci√≥n de gradientes

---

### 9. RESUMEN FINAL (1 minuto)

**Puntos clave a cerrar:**

1. ‚úÖ **√âpocas justificadas:** 200 √©pocas para MNIST alcanzan 91.80% (supera 85%)
2. ‚úÖ **Complejidad validada:** Escalado lineal O(E) confirmado experimentalmente
3. ‚úÖ **Proceso correcto:** Backpropagation verificado (error < 1e-5)
4. ‚úÖ **Decisiones t√©cnicas:** Batch size, learning rate, y arquitectura optimizados
5. ‚úÖ **Resultados:** Consistente superaci√≥n del umbral del 85% en todos los datasets

---

## üìä DATOS CLAVE PARA MEMORIZAR

- **√âpocas base:** 30
- **√âpocas MNIST:** 200
- **Precisi√≥n MNIST:** 91.80%
- **Complejidad temporal:** O(E √ó N √ó (n√óh + h√óc))
- **Complejidad espacial:** O(n√óh + h√óc + B√ó(n+h+c))
- **Batch size:** 32 (base) / 128 (MNIST)
- **Learning rate:** 0.01
- **Error gradientes:** 4.46e-06 (muy por debajo de 1e-5)

---

## üéØ CONSEJOS PARA LA SUSTENTACI√ìN

1. **Empieza con el panorama general:** Arquitectura ‚Üí Entrenamiento ‚Üí Resultados
2. **Enf√≥cate en justificaciones:** No solo digas "200 √©pocas", explica POR QU√â
3. **Usa n√∫meros concretos:** "91.80% de precisi√≥n" es mejor que "m√°s del 90%"
4. **Menciona la complejidad:** Siempre relaciona √©pocas con complejidad temporal
5. **Muestra evidencia:** Referencia a gr√°ficas y experimentos realizados
6. **S√© honesto sobre limitaciones:** Si no implementaste early stopping, dilo y explica por qu√©

---

**¬°√âxito en tu sustentaci√≥n! üöÄ**

